{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only lower-case english letters\n",
    "names_text = open(\"../names.txt\", \"r\").read()\n",
    "\n",
    "words = names_text.splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.emma.', '.olivia.', '.ava.', '.isabella.', '.sophia.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's add start/end special symbol '.' for each word\n",
    "words = [\".\" + word + \".\" for word in words]\n",
    "words[:5]\n",
    "\n",
    "# Later when we train the model based on frequency, it'll be useful to know\n",
    "# what is a popular way to start and end a word\n",
    "\n",
    "# For example '.' -> 'a' is popular because there are a lot of names starting with 'a'\n",
    "# and         'n' -> '.' is popular because there are a lot of names ending with 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's count frequencies\n",
    "def stoi(c):    \n",
    "    if c == '.':\n",
    "        return 0\n",
    "    return ord(c) - ord('a') + 1\n",
    "\n",
    "def itos(i):\n",
    "    if i == 0:\n",
    "        return '.'\n",
    "    return chr(i + ord('a') - 1)\n",
    "\n",
    "\n",
    "# Counting frequencies\n",
    "# freq[0][1] = how many times '.' is followed by 'a'\n",
    "N = torch.zeros(27, 27, dtype=torch.int32)\n",
    "for w in words:\n",
    "    for c, c_next in zip(w, w[1:]):\n",
    "        N[stoi(c), stoi(c_next)] += 1\n",
    "\n",
    "# Plotting the matrix\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for row in range(27):\n",
    "    for col in range(27):\n",
    "        chstr = itos(row) + itos(col)\n",
    "        plt.text(col, row, chstr, ha='center', va='bottom', color='gray')\n",
    "        plt.text(col, row, f'{N[row, col]:.0f}', ha='center', va='top', color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' How do we do it? - step by step:\\n1. Compute 27x1 tensor with sums of rows using N.sum(dim=1, keepdim=True)\\n   dim=1 means that we \"get rid off\" dimension 1 (columns) while summing\\n   keepdim=True means that we keep the dimension 1 in the resulting tensor,\\n   by default it would be squeezed out.\\n\\n2. Copy 27x1 tensor 27 times to get 27x27 tensor\\n\\n3. Now we can divide tensors cell-by-cell.\\n\\nThe copy operation is called broadcasting. It\\'s a powerful mechanism that allows PyTorch to work with arrays of different shapes.\\nTwo tensors are “broadcastable” if the following rules hold:\\n* Each tensor has at least one dimension.\\n* When iterating over the dimension sizes, starting at the trailing dimension, \\n  the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\\n\\nFor example we can add 3x4 tensor to 1x4 tensor because 3 and 1 are compatible.\\nBut we can\\'t add 3x4 tensor to 3x3 tensor because 4 and 3 are not compatible.\\n\\nMore about broadcasting: https://pytorch.org/docs/stable/notes/broadcasting.html \\n\\nWarning!!! Broadcasting is a powerful mechanism but it can be a source of bugs. For example:\\n* N / N.sum(dim=1, keepdim=False) - bug, gives [27] shaped tensor so broadcasting copies rows instead of columns\\n* N / N.sum(dim=1, keepdim=True) - correct!, gives [27, 1] shaped tensor, broadcasting copies columns'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I know that torch.multinomial doesn't need the probabilities to sum to 1\n",
    "# https://pytorch.org/docs/stable/generated/torch.multinomial.html\n",
    "# but let's normalize data because it's a good practice!\n",
    "\n",
    "# We have a matrix of frequencies N and we want to calculate matrix of probabilities.\n",
    "# We want to divide each row by the sum of the row.\n",
    "P = N.float() / N.sum(dim=1, keepdim=True).float()\n",
    "\n",
    "\"\"\" How do we do it? - step by step:\n",
    "1. Compute 27x1 tensor with sums of rows using N.sum(dim=1, keepdim=True)\n",
    "   dim=1 means that we \"get rid off\" dimension 1 (columns) while summing\n",
    "   keepdim=True means that we keep the dimension 1 in the resulting tensor,\n",
    "   by default it would be squeezed out.\n",
    "\n",
    "2. Copy 27x1 tensor 27 times to get 27x27 tensor\n",
    "\n",
    "3. Now we can divide tensors cell-by-cell.\n",
    "\n",
    "The copy operation is called broadcasting. It's a powerful mechanism that allows PyTorch to work with arrays of different shapes.\n",
    "Two tensors are “broadcastable” if the following rules hold:\n",
    "* Each tensor has at least one dimension.\n",
    "* When iterating over the dimension sizes, starting at the trailing dimension, \n",
    "  the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n",
    "\n",
    "For example we can add 3x4 tensor to 1x4 tensor because 3 and 1 are compatible.\n",
    "But we can't add 3x4 tensor to 3x3 tensor because 4 and 3 are not compatible.\n",
    "\n",
    "More about broadcasting: https://pytorch.org/docs/stable/notes/broadcasting.html \n",
    "\n",
    "Warning!!! Broadcasting is a powerful mechanism but it can be a source of bugs. For example:\n",
    "* N / N.sum(dim=1, keepdim=False) - bug, gives [27] shaped tensor so broadcasting copies rows instead of columns\n",
    "* N / N.sum(dim=1, keepdim=True) - correct!, gives [27, 1] shaped tensor, broadcasting copies columns\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hysharandaynnamarieryn\n",
      "alonn\n",
      "feyl\n",
      "rmide\n",
      "briae\n"
     ]
    }
   ],
   "source": [
    "def generate_name_stochastically(probabilities: torch.Tensor) -> str:\n",
    "    \"\"\"\n",
    "    Generate a random name based on character transition frequencies.\n",
    "    Returns a name string with start/end markers removed.\n",
    "    \"\"\"\n",
    "    name = []\n",
    "    idx = 0     # index of the start character\n",
    "    while True:\n",
    "        # Sample the next character based on frequency distribution\n",
    "        p = probabilities[idx]\n",
    "        idx = torch.multinomial(p, 1).item()\n",
    "\n",
    "        # Add the new character to our name\n",
    "        if idx != 0:\n",
    "            name.append(itos(idx))\n",
    "\n",
    "        # If we hit the end marker and desired length, we're done\n",
    "        elif len(name) >= 3:\n",
    "            break\n",
    "\n",
    "    return \"\".join(name)\n",
    "\n",
    "\n",
    "# Generate a few names\n",
    "for _ in range(5):\n",
    "    print(generate_name_stochastically(P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(inf)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GOAL: maximize likelihood of th data w.r.t. the model parameters (statistical modeling)\n",
    "# equivalent to maximizing the log-likelihood (log is a monotonic, increasing function)\n",
    "# equivalent to minimizing the negative log-likelihood (NLL)\n",
    "# equivalent to minimizing the mean negative log-likelihood\n",
    "\n",
    "# log(a*b*c) = log(a) + log(b) + log(c)\n",
    "\n",
    "def compute_loss(probs: torch.Tensor, ys: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the mean negative log-likelihood of a sequence of characters given the transition probabilities.\n",
    "    Add L2 regularization to the loss.\n",
    "\n",
    "    Args:\n",
    "    - probs: a tensor of shape (27, 27) containing transition probabilities\n",
    "    - ys: a tensor of shape (N,) containing character indices. for example:\n",
    "    ys = torch.tensor([0, 1, 2, 3, 0]) corresponds to the sequence '.abcd.'\n",
    "    \"\"\"\n",
    "    loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "    l2_loss = (probs**2).mean() * 0.1  # L2 regularization\n",
    "    return loss + l2_loss\n",
    "\n",
    "\n",
    "ys = torch.tensor([stoi(c) for c in \".andrejq.\"])\n",
    "compute_loss(P, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4629)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't want to have inf in the loss function, so we add a small number to the frequency matrix\n",
    "# it's called Laplace smoothing: https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "P = N.float() + 1.0\n",
    "P /= P.sum(dim=1, keepdim=True)\n",
    "\n",
    "compute_loss(P, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the data into features and labels\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    for c, c_next in zip(w, w[1:]):\n",
    "        xs.append(stoi(c))\n",
    "        ys.append(stoi(c_next))\n",
    "\n",
    "xs, ys = torch.tensor(xs), torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x123d78710>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's create MultiLayerPerceptron model\n",
    "# but how do we encode the input? - one-hot encoding\n",
    "# https://en.wikipedia.org/wiki/One-hot\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "plt.imshow(xenc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize the weights of the model\n",
    "W = torch.randn((27, 27), requires_grad=True)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Normalizes values to a probability distribution using softmax function.\n",
    "    \n",
    "    It's a good way to normalize because:\n",
    "    - the function is monotonic so it doesn't change the order of values\n",
    "    - all values are positive due to exponentiation\n",
    "    - the sum of values is 1 due to normalization\n",
    "    - the function is differentiable so we can use it in backpropagation\"\"\"\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7578442096710205"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "logits = xenc @ W\n",
    "probs = softmax(logits)\n",
    "loss = compute_loss(probs, ys)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "W.grad = None           # clear the gradients\n",
    "loss.backward()         # compute \n",
    "W.data -= 0.1 * W.grad  # update the weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
