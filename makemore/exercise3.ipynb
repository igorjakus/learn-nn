{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Use dev set to tune hyperparameters for Tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.', 'a', 'z')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only lower-case English letters\n",
    "names_text = open(\"names.txt\", \"r\").read()\n",
    "words = [f\"..{name}.\" for name in names_text.splitlines()]\n",
    "\n",
    "# Create encoding and decoding dictionaries\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "ctoi = {c: i for i, c in enumerate(chars)}\n",
    "itoc = {i: c for i, c in enumerate(chars)}\n",
    "itoc[0], itoc[1], itoc[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80, 0.10, 0.10\n"
     ]
    }
   ],
   "source": [
    "# Split the words into train, test, and validation sets\n",
    "np.random.shuffle(words)\n",
    "train_words = words[:int(0.8*len(words))]\n",
    "test_words = words[int(0.8*len(words)):int(0.9*len(words))]\n",
    "dev_words = words[int(0.9*len(words)):]\n",
    "n = len(words)\n",
    "print(f\"{len(train_words) / n:.2f}, {len(test_words) / n:.2f}, {len(dev_words) / n:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramCountingModel():\n",
    "    def __init__(self):\n",
    "        self.N = torch.zeros(len(chars), len(chars), len(chars))\n",
    "        self.P = torch.zeros(len(chars), len(chars), len(chars))\n",
    "\n",
    "    def train(self, train_words: list[str], laplace_alpha: int = 1):\n",
    "        # Count trigrams\n",
    "        for word in train_words:\n",
    "            for i in range(len(word) - 2):\n",
    "                a, b, c = word[i], word[i + 1], word[i + 2]\n",
    "                self.N[ctoi[a], ctoi[b], ctoi[c]] += 1\n",
    "\n",
    "        # Compute probabilities with Laplace smoothing\n",
    "        self.P = (self.N + laplace_alpha).float() / (self.N + laplace_alpha).sum(dim=2, keepdim=True).float()\n",
    "\n",
    "    def compute_loss(self, test_words: list[str]):\n",
    "        \"\"\" Compute negative log-likelihood \"\"\"\n",
    "        nll = 0\n",
    "        n = 0\n",
    "        for word in test_words:\n",
    "            for i in range(len(word) - 2):\n",
    "                n += 1\n",
    "                a, b, c = word[i], word[i + 1], word[i + 2]\n",
    "                nll += -torch.log(self.P[ctoi[a], ctoi[b], ctoi[c]])\n",
    "        return (nll / n).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramNeuralNetwork():\n",
    "    def __init__(self) -> None:\n",
    "        self.W = torch.randn(len(chars) * 2, len(chars), requires_grad=True)\n",
    "\n",
    "    def train(self, words: list[str], epochs=1000, lr=0.1, l2=0.01) -> None:\n",
    "        X_train, y_train = self.words_to_features(words)\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            # forward pass\n",
    "            logits = X_train @ self.W\n",
    "            loss = self.compute_loss(logits, y_train, l2)\n",
    "\n",
    "            # backward pass\n",
    "            self.W.grad = None\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            self.W.data -= lr * self.W.grad\n",
    "    \n",
    "    def compute_loss(self, logits, y_test, alpha) -> float:\n",
    "        cross_entropy = F.cross_entropy(logits, y_test)\n",
    "        l2_regularization = alpha * torch.norm(self.W) ** 2\n",
    "        return cross_entropy + l2_regularization\n",
    "\n",
    "    def test(self, test_words) -> float:\n",
    "        X_test, y_test = self.words_to_features(test_words)\n",
    "        logits = X_test @ self.W\n",
    "        return self.compute_loss(logits, y_test, alpha=0).item()\n",
    "    \n",
    "    def words_to_features(self, words: list[str]) -> tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\" Convert a list of words to encoded features and labels \"\"\"\n",
    "        xs, ys = [], []\n",
    "        for word in words:\n",
    "            for i in range(len(word) - 2):\n",
    "                a, b, c = word[i], word[i + 1], word[i + 2]\n",
    "                xs.append((ctoi[a], ctoi[b]))\n",
    "                ys.append(ctoi[c])\n",
    "        return self.encode(torch.tensor(xs)), torch.tensor(ys)\n",
    "\n",
    "    \n",
    "    def encode(self, X: torch.tensor) -> torch.tensor:\n",
    "        \"\"\" One-hot encode the input \"\"\"\n",
    "        xenc = torch.zeros(len(X), len(chars) * 2)\n",
    "        for i, (a, b) in enumerate(X):\n",
    "            aenc = F.one_hot(a, num_classes=len(chars))\n",
    "            benc = F.one_hot(b, num_classes=len(chars))\n",
    "            xenc[i] = torch.cat((aenc, benc)).float()\n",
    "        return xenc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev models (validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3787693977355957\n"
     ]
    }
   ],
   "source": [
    "model = TrigramCountingModel()\n",
    "model.train(dev_words, laplace_alpha=1)  # best alpha is 1\n",
    "loss = model.compute_loss(test_words)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3691909313201904\n"
     ]
    }
   ],
   "source": [
    "model = TrigramNeuralNetwork()\n",
    "model.train(dev_words, epochs=500, lr=30, l2=0.00001)  # quite good hyperparameters\n",
    "loss = model.test(test_words)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.24503755569458\n"
     ]
    }
   ],
   "source": [
    "model = TrigramCountingModel()\n",
    "model.train(train_words, laplace_alpha=1)  # best alpha is 1\n",
    "loss = model.compute_loss(test_words)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3527660369873047\n"
     ]
    }
   ],
   "source": [
    "model = TrigramNeuralNetwork()\n",
    "model.train(train_words, epochs=500, lr=30, l2=0.00001)  # quite good hyperparameters\n",
    "loss = model.test(test_words)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOSS:\n",
    "* random:         ~4.4\n",
    "* counting model: 2.25\n",
    "* neural network: 2.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
