{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Meta-exercise. Think of fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My exercise is to change `names.txt` to `pokemons.txt` and try to create new pokemon name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '.',\n",
       " 1: ' ',\n",
       " 2: \"'\",\n",
       " 3: '-',\n",
       " 4: '2',\n",
       " 5: 'a',\n",
       " 6: 'b',\n",
       " 7: 'c',\n",
       " 8: 'd',\n",
       " 9: 'e',\n",
       " 10: 'f',\n",
       " 11: 'g',\n",
       " 12: 'h',\n",
       " 13: 'i',\n",
       " 14: 'j',\n",
       " 15: 'k',\n",
       " 16: 'l',\n",
       " 17: 'm',\n",
       " 18: 'n',\n",
       " 19: 'o',\n",
       " 20: 'p',\n",
       " 21: 'q',\n",
       " 22: 'r',\n",
       " 23: 's',\n",
       " 24: 't',\n",
       " 25: 'u',\n",
       " 26: 'v',\n",
       " 27: 'w',\n",
       " 28: 'x',\n",
       " 29: 'y',\n",
       " 30: 'z',\n",
       " 31: 'é',\n",
       " 32: '♀',\n",
       " 33: '♂'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only lower-case English letters\n",
    "names_text = open(\"pokemons.txt\", \"r\").read()\n",
    "words = [f\".{name.lower()}.\" for name in names_text.splitlines()]  # cleaned up names manually so there's no '.' in the names\n",
    "\n",
    "# Create encoding and decoding dictionaries\n",
    "chars = ['.'] + sorted(list(set(\"\".join(words)) - set(['.'])))\n",
    "ctoi = {c: i for i, c in enumerate(chars)}\n",
    "itoc = {i: c for i, c in enumerate(chars)}\n",
    "itoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80, 0.10, 0.10\n"
     ]
    }
   ],
   "source": [
    "# Split the words into train, test, and validation sets\n",
    "np.random.shuffle(words)\n",
    "train_words = words[:int(0.8*len(words))]\n",
    "test_words = words[int(0.8*len(words)):int(0.9*len(words))]\n",
    "dev_words = words[int(0.9*len(words)):]\n",
    "n = len(words)\n",
    "print(f\"{len(train_words) / n:.2f}, {len(test_words) / n:.2f}, {len(dev_words) / n:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramCountingModel():\n",
    "    def __init__(self):\n",
    "        self.N = torch.zeros(len(chars), len(chars))\n",
    "        self.P = torch.zeros(len(chars), len(chars))\n",
    "\n",
    "    def train(self, train_words: list[str], laplace_alpha: int = 1):\n",
    "        # Count trigrams\n",
    "        for word in train_words:\n",
    "            for i in range(len(word) - 1):\n",
    "                a, b = word[i], word[i + 1]\n",
    "                self.N[ctoi[a], ctoi[b]] += 1\n",
    "\n",
    "        # Compute probabilities with Laplace smoothing\n",
    "        self.P = (self.N + laplace_alpha).float() / (self.N + laplace_alpha).sum(dim=1, keepdim=True).float()\n",
    "\n",
    "    def compute_loss(self, test_words: list[str]):\n",
    "        \"\"\" Compute negative log-likelihood \"\"\"\n",
    "        nll = 0\n",
    "        n = 0\n",
    "        for word in test_words:\n",
    "            for i in range(len(word) - 1):\n",
    "                n += 1\n",
    "                a, b = word[i], word[i + 1]\n",
    "                nll += -torch.log(self.P[ctoi[a], ctoi[b]])\n",
    "        return (nll / n).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramNeuralNetwork():\n",
    "    def __init__(self) -> None:\n",
    "        self.W = torch.randn(len(chars), len(chars), requires_grad=True)\n",
    "\n",
    "    def train(self, train_words: list[str], epochs=1000, lr=0.1, l2=0.01) -> None:\n",
    "        X_train, y_train = self.words_to_features(train_words)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            # forward pass\n",
    "            logits = self.forward(X_train)\n",
    "            loss = self.compute_loss(logits, y_train, l2)\n",
    "\n",
    "            # backward pass\n",
    "            self.W.grad = None\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            self.W.data -= lr * self.W.grad\n",
    "    \n",
    "    def compute_loss(self, logits: torch.tensor, y_test: torch.tensor, alpha: float) -> float:\n",
    "        cross_entropy = F.cross_entropy(logits, y_test)\n",
    "        l2_regularization = alpha * torch.norm(self.W) ** 2\n",
    "        return cross_entropy + l2_regularization\n",
    "\n",
    "    def forward(self, X: torch.tensor) -> torch.tensor:\n",
    "        return self.W[X]  # instead of matrix multiplication, we use fancy indexing\n",
    "\n",
    "    def test(self, test_words: list[str]) -> float:\n",
    "        X_test, y_test = self.words_to_features(test_words)\n",
    "        logits = self.forward(X_test)\n",
    "        return self.compute_loss(logits, y_test, alpha=0).item()\n",
    "    \n",
    "    def words_to_features(self, words: list[str]) -> tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\" Convert a list of words to features and labels \"\"\"\n",
    "        xs, ys = [], []\n",
    "        for word in words:\n",
    "            for i in range(len(word) - 1):\n",
    "                a, b = word[i], word[i + 1]\n",
    "                xs.append(ctoi[a])\n",
    "                ys.append(ctoi[b])\n",
    "\n",
    "        xs = torch.tensor(xs)\n",
    "        ys = torch.tensor(ys)\n",
    "        return xs, ys\n",
    "    \n",
    "    def generate(self) -> str:\n",
    "        idx = ctoi[\".\"]\n",
    "        generated = []\n",
    "        while len(generated) < 3 or generated[-1] != \".\":\n",
    "            probs = F.softmax(self.W[idx], dim=0)\n",
    "            idx = torch.multinomial(probs, 1).item()\n",
    "            generated += itoc[idx]\n",
    "        return \"\".join(generated[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev models (validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.984520196914673\n"
     ]
    }
   ],
   "source": [
    "model = BigramCountingModel()\n",
    "model.train(dev_words, laplace_alpha=1)  # best alpha is 1\n",
    "loss = model.compute_loss(test_words)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.041950225830078\n"
     ]
    }
   ],
   "source": [
    "model = BigramNeuralNetwork()\n",
    "model.train(dev_words, epochs=500, lr=30, l2=0.00001)  # quite good hyperparameters\n",
    "loss = model.test(test_words)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.725548267364502\n"
     ]
    }
   ],
   "source": [
    "model = BigramCountingModel()\n",
    "model.train(train_words, laplace_alpha=1)  # best alpha is 1\n",
    "loss = model.compute_loss(test_words)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.715907573699951\n"
     ]
    }
   ],
   "source": [
    "model = BigramNeuralNetwork()\n",
    "model.train(train_words, epochs=500, lr=30, l2=0.00001)  # quite good hyperparameters\n",
    "loss = model.test(test_words)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new Pokemon names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'angiwaloct'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
