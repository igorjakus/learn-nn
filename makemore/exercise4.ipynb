{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Delete 1-hot and use indexing instead of matrix mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.', 'a', 'z')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only lower-case English letters\n",
    "names_text = open(\"names.txt\", \"r\").read()\n",
    "words = [f\".{name}.\" for name in names_text.splitlines()]\n",
    "\n",
    "# Create encoding and decoding dictionaries\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "ctoi = {c: i for i, c in enumerate(chars)}\n",
    "itoc = {i: c for i, c in enumerate(chars)}\n",
    "itoc[0], itoc[1], itoc[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00, 0.10, 0.10\n"
     ]
    }
   ],
   "source": [
    "# Split the words into train, test, and validation sets\n",
    "np.random.shuffle(words)\n",
    "train_words = words[:int(0.8*len(words))]\n",
    "test_words = words[int(0.8*len(words)):int(0.9*len(words))]\n",
    "dev_words = words[int(0.9*len(words)):]\n",
    "n = len(words)\n",
    "print(f\"{len(train_words) / n:.2f}, {len(test_words) / n:.2f}, {len(dev_words) / n:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramCountingModel():\n",
    "    def __init__(self):\n",
    "        self.N = torch.zeros(len(chars), len(chars))\n",
    "        self.P = torch.zeros(len(chars), len(chars))\n",
    "\n",
    "    def train(self, train_words: list[str], laplace_alpha: int = 1):\n",
    "        # Count trigrams\n",
    "        for word in train_words:\n",
    "            for i in range(len(word) - 1):\n",
    "                a, b = word[i], word[i + 1]\n",
    "                self.N[ctoi[a], ctoi[b]] += 1\n",
    "\n",
    "        # Compute probabilities with Laplace smoothing\n",
    "        self.P = (self.N + laplace_alpha).float() / (self.N + laplace_alpha).sum(dim=1, keepdim=True).float()\n",
    "\n",
    "    def compute_loss(self, test_words: list[str]):\n",
    "        \"\"\" Compute negative log-likelihood \"\"\"\n",
    "        nll = 0\n",
    "        n = 0\n",
    "        for word in test_words:\n",
    "            for i in range(len(word) - 1):\n",
    "                n += 1\n",
    "                a, b = word[i], word[i + 1]\n",
    "                nll += -torch.log(self.P[ctoi[a], ctoi[b]])\n",
    "        return (nll / n).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramNeuralNetwork():\n",
    "    def __init__(self) -> None:\n",
    "        self.W = torch.randn(len(chars), len(chars), requires_grad=True)\n",
    "\n",
    "    def train(self, train_words: list[str], epochs=1000, lr=0.1, l2=0.01) -> None:\n",
    "        X_train, y_train = self.words_to_features(train_words)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            # forward pass\n",
    "            logits = self.forward(X_train)\n",
    "            loss = self.compute_loss(logits, y_train, l2)\n",
    "\n",
    "            # backward pass\n",
    "            self.W.grad = None\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            self.W.data -= lr * self.W.grad\n",
    "    \n",
    "    def compute_loss(self, logits: torch.tensor, y_test: torch.tensor, alpha: float) -> float:\n",
    "        cross_entropy = F.cross_entropy(logits, y_test)\n",
    "        l2_regularization = alpha * torch.norm(self.W) ** 2\n",
    "        return cross_entropy + l2_regularization\n",
    "\n",
    "    def forward(self, X: torch.tensor) -> torch.tensor:\n",
    "        return self.W[X]  # instead of matrix multiplication, we use fancy indexing\n",
    "\n",
    "    def test(self, test_words: list[str]) -> float:\n",
    "        X_test, y_test = self.words_to_features(test_words)\n",
    "        logits = self.forward(X_test)\n",
    "        return self.compute_loss(logits, y_test, alpha=0).item()\n",
    "    \n",
    "    def words_to_features(self, words: list[str]) -> tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\" Convert a list of words to features and labels \"\"\"\n",
    "        xs, ys = [], []\n",
    "        for word in words:\n",
    "            for i in range(len(word) - 1):\n",
    "                a, b = word[i], word[i + 1]\n",
    "                xs.append(ctoi[a])\n",
    "                ys.append(ctoi[b])\n",
    "\n",
    "        xs = torch.tensor(xs)\n",
    "        ys = torch.tensor(ys)\n",
    "        return xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev models (validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.473254680633545\n"
     ]
    }
   ],
   "source": [
    "model = BigramCountingModel()\n",
    "model.train(dev_words, laplace_alpha=1)  # best alpha is 1\n",
    "loss = model.compute_loss(test_words)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.471043348312378\n"
     ]
    }
   ],
   "source": [
    "model = BigramNeuralNetwork()\n",
    "model.train(dev_words, epochs=500, lr=30, l2=0.00001)  # quite good hyperparameters\n",
    "loss = model.test(test_words)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4520514011383057\n"
     ]
    }
   ],
   "source": [
    "model = BigramCountingModel()\n",
    "model.train(train_words, laplace_alpha=1)  # best alpha is 1\n",
    "loss = model.compute_loss(test_words)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.459134578704834\n"
     ]
    }
   ],
   "source": [
    "model = BigramNeuralNetwork()\n",
    "model.train(train_words, epochs=500, lr=30, l2=0.00001)  # quite good hyperparameters\n",
    "loss = model.test(test_words)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOSS:\n",
    "* random:         ~4.4\n",
    "* counting model: 2.45\n",
    "* neural network: 2.46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
