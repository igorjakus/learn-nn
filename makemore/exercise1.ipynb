{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Create Tri-gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..emma.', '..olivia.', '..ava.', '..isabella.', '..sophia.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only lower-case English letters\n",
    "names_text = open(\"../names.txt\", \"r\").read()\n",
    "words = [f\"..{name}.\" for name in names_text.splitlines()]\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, '.', 2, 'b')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat encoding and decoding dictionaries\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "ctoi = {c: i for i, c in enumerate(chars)}\n",
    "itoc = {i: c for i, c in enumerate(chars)}\n",
    "ctoi['.'], itoc[0], ctoi['b'], itoc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(805.), tensor(1.))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = torch.zeros(len(chars), len(chars), len(chars))  # Tri-gram\n",
    "for word in words:\n",
    "    for i in range(len(word) - 2):\n",
    "        # count how many times 3 characters appear together\n",
    "        a, b, c = word[i], word[i + 1], word[i + 2]\n",
    "        N[ctoi[a], ctoi[b], ctoi[c]] += 1\n",
    "\n",
    "N = N + 1  # Laplace smoothing\n",
    "\n",
    "N[ctoi['a'], ctoi['n'], ctoi['a']], N[ctoi['x'], ctoi['q'], ctoi['w']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the tri-gram matrix to get the probability\n",
    "N = N.float()  # convert to float\n",
    "P = N / N.sum(dim=2, keepdim=True)  # we want P[i][j].sum() == 1\n",
    "P[14, 23].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1473), tensor(0.0370))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[ctoi['a'], ctoi['n'], ctoi['a']], P[ctoi['x'], ctoi['q'], ctoi['w']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aishmatcvone\n",
      "ez\n",
      "jaylakenyle\n",
      "amylenne\n",
      "landiti\n"
     ]
    }
   ],
   "source": [
    "def generate_name_stochastically(P):\n",
    "    name = \"..\"\n",
    "    while True:\n",
    "        i, j = ctoi[name[-2]], ctoi[name[-1]]\n",
    "        k = torch.multinomial(P[i][j], 1).item()\n",
    "        name += itoc[k]\n",
    "        if name[-1] == \".\":\n",
    "            break\n",
    "    return name[2:-1]\n",
    "\n",
    "for i in range(5):\n",
    "    name = generate_name_stochastically(P)\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2120)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's calculate the mean of negative log likelihood (loss function)\n",
    "nll = 0\n",
    "n = 1\n",
    "for word in words:\n",
    "    for i in range(len(word) - 2):\n",
    "        a, b, c = word[i], word[i + 1], word[i + 2]\n",
    "        n += 1\n",
    "        likelihood = P[ctoi[a], ctoi[b], ctoi[c]]\n",
    "        nll -= torch.log(likelihood)\n",
    "\n",
    "nll / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 2]), torch.Size([228146]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into features and labels\n",
    "xs, ys = [], []\n",
    "for word in words:\n",
    "    for i in range(len(word) - 2):\n",
    "        a, b, c = word[i], word[i + 1], word[i + 2]\n",
    "        xs.append((ctoi[a], ctoi[b]))\n",
    "        ys.append(ctoi[c])\n",
    "\n",
    "xs, ys = torch.tensor(xs), torch.tensor(ys)\n",
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 54])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the features using one-hot encoding\n",
    "xenc = torch.zeros(len(xs), len(chars) * 2)\n",
    "for i, (a, b) in enumerate(xs):\n",
    "    aenc = F.one_hot(a, num_classes=len(chars))\n",
    "    benc = F.one_hot(b, num_classes=len(chars))\n",
    "    xenc[i] = torch.cat((aenc, benc)).float()\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:  [batch_size, chars], \n",
    "# W: [2*chars, chars]\n",
    "# Output: [batch_size, chars]\n",
    "# [batch_size x (2*chars)] @ [2*chars x chars] = batch_size x chars\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.W = torch.randn(len(chars) * 2, len(chars), requires_grad=True)\n",
    "        self.loss = 0\n",
    "\n",
    "    def train(self, xenc: torch.Tensor, ys: torch.Tensor, epochs=1000, lr=0.1):\n",
    "        for _ in range(epochs):\n",
    "            # forward pass\n",
    "            logits = xenc @ self.W\n",
    "            # probs = F.softmax(logits, dim=1)\n",
    "            # self.loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "            self.loss = F.cross_entropy(logits, ys)  # equivalent to above 2 lines\n",
    "\n",
    "            # backward pass\n",
    "            self.W.grad = None\n",
    "            self.loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            self.W.data -= lr * self.W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3414723873138428"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.train(xenc, ys, epochs=600, lr=30)\n",
    "model.loss.item()\n",
    "\n",
    "# LOSS:\n",
    "# random:         4.16\n",
    "# counting model: 2.21\n",
    "# neural network: 2.34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
