{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Split dataset into train/test/dev and use them on Tri-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..emma.', '..olivia.', '..ava.', '..isabella.', '..sophia.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only lower-case English letters\n",
    "names_text = open(\"names.txt\", \"r\").read()\n",
    "words = [f\"..{name}.\" for name in names_text.splitlines()]\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, '.', 2, 'b')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat encoding and decoding dictionaries\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "ctoi = {c: i for i, c in enumerate(chars)}\n",
    "itoc = {i: c for i, c in enumerate(chars)}\n",
    "ctoi['.'], itoc[0], ctoi['b'], itoc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80, 0.10, 0.10\n"
     ]
    }
   ],
   "source": [
    "# Split the words into train, test, and validation sets\n",
    "np.random.shuffle(words)\n",
    "train_words = words[:int(0.8*len(words))]\n",
    "test_words = words[int(0.8*len(words)):int(0.9*len(words))]\n",
    "dev_words = words[int(0.9*len(words)):]\n",
    "n = len(words)\n",
    "print(f\"{len(train_words) / n:.2f}, {len(test_words) / n:.2f}, {len(dev_words) / n:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(641.), tensor(1.))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = torch.zeros(len(chars), len(chars), len(chars))  # Tri-gram\n",
    "for word in train_words:\n",
    "    for i in range(len(word) - 2):\n",
    "        # count how many times 3 characters appear together\n",
    "        a, b, c = word[i], word[i + 1], word[i + 2]\n",
    "        N[ctoi[a], ctoi[b], ctoi[c]] += 1\n",
    "\n",
    "N = N + 1  # Laplace smoothing\n",
    "\n",
    "N[ctoi['a'], ctoi['n'], ctoi['a']], N[ctoi['x'], ctoi['q'], ctoi['w']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the tri-gram matrix to get the probability\n",
    "N = N.float()  # convert to float\n",
    "P = N / N.sum(dim=2, keepdim=True)  # we want P[i][j].sum() == 1\n",
    "P[14, 23].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1446), tensor(0.0370))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[ctoi['a'], ctoi['n'], ctoi['a']], P[ctoi['x'], ctoi['q'], ctoi['w']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seveven\n",
      "luile\n",
      "mandreyracist\n",
      "yia\n",
      "ris\n"
     ]
    }
   ],
   "source": [
    "def generate_name_stochastically(P):\n",
    "    name = \"..\"\n",
    "    while True:\n",
    "        i, j = ctoi[name[-2]], ctoi[name[-1]]\n",
    "        k = torch.multinomial(P[i][j], 1).item()\n",
    "        name += itoc[k]\n",
    "        if name[-1] == \".\":\n",
    "            break\n",
    "    return name[2:-1]\n",
    "\n",
    "for i in range(5):\n",
    "    name = generate_name_stochastically(P)\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2466)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's calculate the mean of negative log likelihood (loss function)\n",
    "nll = 0\n",
    "n = 1\n",
    "for word in test_words:\n",
    "    for i in range(len(word) - 2):\n",
    "        a, b, c = word[i], word[i + 1], word[i + 2]\n",
    "        n += 1\n",
    "        likelihood = P[ctoi[a], ctoi[b], ctoi[c]]\n",
    "        nll -= torch.log(likelihood)\n",
    "\n",
    "nll / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182585, 2]), torch.Size([182585]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into features and labels\n",
    "def words_to_features(words):\n",
    "    xs, ys = [], []\n",
    "    for word in words:\n",
    "        for i in range(len(word) - 2):\n",
    "            a, b, c = word[i], word[i + 1], word[i + 2]\n",
    "            xs.append((ctoi[a], ctoi[b]))\n",
    "            ys.append(ctoi[c])\n",
    "    return torch.tensor(xs), torch.tensor(ys)\n",
    "\n",
    "\n",
    "xs_train, ys_train = words_to_features(train_words)\n",
    "xs_train.shape, ys_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the features using one-hot encoding\n",
    "def encode(x):\n",
    "    xenc = torch.zeros(len(x), len(chars) * 2)\n",
    "    for i, (a, b) in enumerate(x):\n",
    "        aenc = F.one_hot(a, num_classes=len(chars))\n",
    "        benc = F.one_hot(b, num_classes=len(chars))\n",
    "        xenc[i] = torch.cat((aenc, benc)).float()\n",
    "    return xenc\n",
    "\n",
    "xenc_train = encode(xs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:  [batch_size, chars], \n",
    "# W: [2*chars, chars]\n",
    "# Output: [batch_size, chars]\n",
    "# [batch_size x (2*chars)] @ [2*chars x chars] = batch_size x chars\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.W = torch.randn(len(chars) * 2, len(chars), requires_grad=True)\n",
    "        self.loss = 0\n",
    "\n",
    "    def train(self, xenc: torch.Tensor, ys: torch.Tensor, epochs=1000, lr=0.1):\n",
    "        for _ in range(epochs):\n",
    "            # forward pass\n",
    "            logits = xenc @ self.W\n",
    "            # probs = F.softmax(logits, dim=1)\n",
    "            # self.loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "            self.loss = F.cross_entropy(logits, ys)  # equivalent to above 2 lines\n",
    "\n",
    "            # backward pass\n",
    "            self.W.grad = None\n",
    "            self.loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            self.W.data -= lr * self.W.grad\n",
    "\n",
    "    def test(self, xenc: torch.Tensor, ys: torch.Tensor) -> float:\n",
    "        logits = xenc @ self.W\n",
    "        return F.cross_entropy(logits, ys).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.355668067932129"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.train(xenc_train, ys_train, epochs=600, lr=30)\n",
    "\n",
    "x_test, y_test = words_to_features(test_words)\n",
    "xenc_test = encode(x_test)\n",
    "model.test(xenc_test, y_test)\n",
    "\n",
    "# LOSS:\n",
    "# random:         4.16\n",
    "# counting model: 2.21\n",
    "# neural network: 2.34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
